{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import hmean, gmean\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances as cosine_distances, cosine_similarity\n",
    "\n",
    "path = \"/global/cscratch1/sd/ajaybati/bertmodelDSe3.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "# model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(p,r):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_list = []\n",
    "    for index in range(len(p)):\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu(p[index],r[index],smoothing_function=smoothie)\n",
    "        bleu_list.append(BLEUscore)\n",
    "    return sum(bleu_list) / len(bleu_list)\n",
    "def getSent_pred(prediction,real_labels): #convert all ids to sentences for bscore\n",
    "    sentlist_real = []\n",
    "    sep_list = []\n",
    "    for sent2 in real_labels:\n",
    "        tokenized = tokenizer.convert_ids_to_tokens(sent2)\n",
    "        sep = tokenized.index('[SEP]')\n",
    "        sep_list.append(sep)\n",
    "        sentlist_real.append(tokenized[1:sep])\n",
    "    \n",
    "    \n",
    "    sentlist_ids = []\n",
    "    sentlist = []\n",
    "    for sent in prediction:\n",
    "        word_list = []\n",
    "        for word in sent:\n",
    "            word_list.append(torch.argmax(word))\n",
    "        sentlist_ids.append(word_list)\n",
    "    \n",
    "    for index,sent in enumerate(sentlist_ids):\n",
    "        sentlist.append(tokenizer.convert_ids_to_tokens(sent)[1:sep_list[index]])\n",
    "    return sentlist,sentlist_real\n",
    "def calc_accuracy(prediction, real_labels, mask_indices):\n",
    "    score = 0\n",
    "    total = 0\n",
    "    for step,sent in enumerate(mask_indices):\n",
    "        if list(sent).count(0)!=40:\n",
    "            for mask in sent:\n",
    "                if int(mask)!=0:\n",
    "                    predicted_index = int(torch.argmax(prediction[step,int(mask)]))\n",
    "                    actual = int(real_labels[step][int(mask)])\n",
    "                    if bool(predicted_index==actual):\n",
    "                        score+=1\n",
    "                    total+=1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    p,r = getSent_pred(prediction,real_labels)\n",
    "    \n",
    "    \n",
    "    accuracy = score/total\n",
    "    try:\n",
    "        bscore = bleu(p,r)\n",
    "    except:\n",
    "        bscore = \"Unfortunately, not possible\"\n",
    "    return accuracy, bscore \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/global/cscratch1/sd/ajaybati/model_ckptDS5.pickle' #path for saved model\n",
    "def load_model(): #load trained model for inference\n",
    "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu'))['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model_input(sents,n_percent_mask=0.0):\n",
    "    input_ids_real = []\n",
    "    att = []\n",
    "    compare = []\n",
    "    mask_indices = []\n",
    "    for sent in sent_tokenize(sents):\n",
    "        mask = []\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,                      # Sentence to encode.\n",
    "            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "            max_length = 128,\n",
    "            truncation = True,# Pad & truncate all sentences.\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,   # Construct attn. masks.\n",
    "            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "       )\n",
    "        input_ids = encoded_dict['input_ids']\n",
    "        compare.append(input_ids)\n",
    "        attention_masks = encoded_dict['attention_mask']\n",
    "        att.append(attention_masks)\n",
    "        input_ids_part = []\n",
    "        for step,word in enumerate(input_ids[0]):\n",
    "            if int(word) != 101 and int(word) != 102:\n",
    "                rando = random.random()\n",
    "                random.seed()\n",
    "                if rando < n_percent_mask and int(word)!=0:\n",
    "                    mask.append(step)\n",
    "                    input_ids_part.append(103)\n",
    "                else:\n",
    "                    input_ids_part.append(int(word))\n",
    "            else:\n",
    "                input_ids_part.append(int(word))\n",
    "        input_ids_part = torch.tensor(input_ids_part).view(1,128)\n",
    "        input_ids_real.append(input_ids_part)\n",
    "        mask_indices.append(mask)\n",
    "\n",
    "    if len(input_ids_real)>1:\n",
    "        bert_input = torch.cat(tuple(input_ids_real),0)\n",
    "        att = torch.cat(tuple(att),0)\n",
    "        compare = torch.cat(tuple(compare),0)\n",
    "    else:\n",
    "        bert_input = input_ids_real[0]\n",
    "        att = att[0]\n",
    "        compare = compare[0]\n",
    "\n",
    "    return bert_input, att, compare, mask_indices, tokenizer.tokenize(sent)\n",
    "\n",
    "def bert_model_output(model, bert_input, att, compare, mask_indices):\n",
    "    loss, predictions = model(bert_input,attention_mask = att, masked_lm_labels = compare)\n",
    "    accuracy, bscore = calc_accuracy(predictions, compare, mask_indices)\n",
    "\n",
    "    return {\"loss\":loss,\n",
    "            \"predictions\": predictions,\n",
    "            \"performance\":[accuracy, bscore]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======ALIGNMENT FUNCTIONS (expensive search)========\n",
    "def distance_matrix(s1, s2):\n",
    "    '''\n",
    "    Method calcualtes cosine distance matrix between two sequences. Returns\n",
    "    distances between values of -1.0 and 1.0.\n",
    "\n",
    "    '''\n",
    "    a = 1 - cdist(s1,s2,'cosine')\n",
    "    return a\n",
    "\n",
    "def scoring_matrix(a, wi=1.0, wj=1.0, epsilon=0.01):\n",
    "    '''\n",
    "    Method generates scoring matrix used to align sequences. This algorithm is\n",
    "    inspired by the Smith-Waterman local sequence-alignment algorithm used\n",
    "    in bioinformatics. Source: https://en.wikipedia.org/wiki/Smith–Waterman_algorithm\n",
    "\n",
    "    The gap weights are adpatively assigned according to fuzzy graph kernels defined\n",
    "    by wi, wj and eplison. Gap weights vary from (0.0, 0.0) to (wi, wj) where\n",
    "    small gaps are closer to 0.0.\n",
    "\n",
    "    '''m\n",
    "    # Pad distance matrix\n",
    "    sa = np.pad(a, ((1,0),(1,0)), 'constant', constant_values=0)\n",
    "\n",
    "    # Calculate gap weight kernels\n",
    "    dims = a.shape\n",
    "    wi_ = [wi*np.exp((i*np.log(epsilon))/dims[0]) for i in reversed(range(dims[0]+1))]\n",
    "    wj_ = [wj*np.exp((j*np.log(epsilon))/dims[1]) for j in reversed(range(dims[1]+1))]\n",
    "\n",
    "    # Updated scoring matrix according to policy\n",
    "    for i in range(1,dims[0]+1):\n",
    "        for j in range(1,dims[1]+1):\n",
    "\n",
    "            inputs = [(sa[i,j]+sa[i-1,j-1]), # Top Left + Bottom Right\n",
    "                      np.max(sa[:i,j])-wi_[i-np.argmax(sa[:i,j])], # Max of all previous values in column - column gap weight\n",
    "                      np.max(sa[i,:j])-wj_[j-np.argmax(sa[i,:j])], # Max of all previous values in row - row gap weight\n",
    "                      0] # Zero\n",
    "            sa[i,j] = np.max(inputs)\n",
    "    return sa\n",
    "\n",
    "def traceback(sa, k=100):\n",
    "    '''\n",
    "    Method preforms traceback path finding on scoring matrix to find first k alignments\n",
    "    of length greater than 1.\n",
    "\n",
    "    '''\n",
    "    # Sort scoring matrix values in descending order; Save coordinates in look up table.\n",
    "    sorted_args = np.argsort(sa.flatten())[::-1]\n",
    "    coords = [(i,j) for i in range(sa.shape[0]) for j in range(sa.shape[1])]\n",
    "\n",
    "    # Perform traceback until all coords have been visted\n",
    "    tracebacks = []\n",
    "    seen = []\n",
    "    route = []\n",
    "    for ind in sorted_args:\n",
    "        i, j = coords[ind]\n",
    "\n",
    "        flag = True\n",
    "        score = sa[i,j]\n",
    "        while(flag):\n",
    "\n",
    "            # Route connects to other traceback\n",
    "            if (i,j) in seen:\n",
    "                tracebacks.append([route,(i,j)])\n",
    "                route = []\n",
    "                break\n",
    "\n",
    "            route.append((i,j))\n",
    "            seen.append((i,j))\n",
    "\n",
    "            # Route terminates at zero\n",
    "            if sa[i,j] == 0:\n",
    "                tracebacks.append([route,[]])\n",
    "                route = []\n",
    "                break\n",
    "\n",
    "            # Select path direction\n",
    "            kernel = [sa[i-1,j],sa[i,j-1],sa[i-1,j-1],sa[i,j]]\n",
    "            m = np.argmax(kernel)\n",
    "\n",
    "            # Move to next gap\n",
    "            if m == 0:\n",
    "                # Terminate route if score is less than gap value\n",
    "                if score > sa[i-1,j]:\n",
    "                    i -= 1\n",
    "                    score += sa[i,j]\n",
    "                else:\n",
    "                    tracebacks.append([route,[]])\n",
    "                    route = []\n",
    "                    break\n",
    "            elif m==1:\n",
    "                # Terminate route if score is less than gap value\n",
    "                if score > sa[i,j-1]:\n",
    "                    j -= 1\n",
    "                    score += sa[i,j]\n",
    "                else:\n",
    "                    tracebacks.append([route,[]])\n",
    "                    route = []\n",
    "                    break\n",
    "\n",
    "            # Move to next hit\n",
    "            elif m==2:\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "                score += sa[i,j]\n",
    "            elif m==3:\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "                score += sa[i,j]\n",
    "​\n",
    "    # Return alignments with length greater than 1 in order as discovered\n",
    "    if k == None: k = len(tracebacks)\n",
    "    alignments = []\n",
    "    for _ in tracebacks:\n",
    "        if len(_[0]) > 1:\n",
    "            r = [(i-1,j-1) for i,j in _[0]]\n",
    "            alignments.append(r)\n",
    "        if len(alignments) == k: break\n",
    "\n",
    "    return alignments\n",
    "\n",
    "def score_alignment(alignment, s1, s2, k):\n",
    "    '''\n",
    "    This method is used to calculate a global score for aligmnets, to sort\n",
    "    alignments from multiple search queries of the same topic. This is still\n",
    "    a work in progress, but has shown good prelimanary results on the note example.\n",
    "\n",
    "    '''\n",
    "    # Find gaps and hits, and gather feature vectors\n",
    "    temp_i = []\n",
    "    temp_j = []\n",
    "    i = -1\n",
    "    j = -1\n",
    "    s1_ = []\n",
    "    s2_ = []\n",
    "    for _ in alignment:\n",
    "        if _[0] != i:\n",
    "            temp_i.append(1)\n",
    "            i = _[0]\n",
    "        else: temp_i.append(0.0)\n",
    "        if _[1] != j:\n",
    "            temp_j.append(1)\n",
    "            j = _[1]\n",
    "        else: temp_j.append(0.0)\n",
    "        s1_.append(s1[_[0]])\n",
    "        s2_.append(s2[_[1]])\n",
    "\n",
    "    # Calculate similarity score\n",
    "    mask = np.array(temp_i) * np.array(temp_j)\n",
    "    similarity = 2 - cdist(s1_,s2_,'cosine').diagonal()\n",
    "    score = hmean([k , ((similarity/2)*mask).mean() , (mask.sum()/len(alignment))])\n",
    "\n",
    "    return score\n",
    "\n",
    "#===== RUN INFERENCE and ALIGN\n",
    "\n",
    "model = load_model()\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output[0].detach()\n",
    "    return hook\n",
    "model.bert.encoder.register_forward_hook(get_activation(\"encoder\")) # Adds hook to get ouput from internediate layer\n",
    "\n",
    "# Get internediate Representations For these Two Passages\n",
    "str1=\"The rabbit-hole went straight on like a tunnel for some way,\\\n",
    "and then dipped suddenly down, so suddenly that Alice had not a\\\n",
    "moment to think about stopping herself before she found herself\\\n",
    "falling down a very deep well.\"\n",
    "\n",
    "model_input, att, compare, mask_indices, tokens1 = get_model_input(str1, n_percent_mask=0.0)\n",
    "out = model(model_input,attention_mask = att, masked_lm_labels = compare)\n",
    "out1 = activation[\"encoder\"].numpy()[0][0:len(tokens1)] # Gets tensor values for first sentence\n",
    "# Will need to be changed to include more.\n",
    "# The alignment takes in 2 numpy array of size N*F where N is the number of words\n",
    "# and F is the number of embedding features.\n",
    "\n",
    "str2=\"a bunny dug in the ground\" # Search Criteria\n",
    "\n",
    "model_input, att, compare, mask_indices, tokens2 = get_model_input(str2, n_percent_mask=0.0)\n",
    "out = model(model_input,attention_mask = att, masked_lm_labels = compare)\n",
    "out2 = activation[\"encoder\"].numpy()[0][0:len(tokens2)]\n",
    "\n",
    "# Search for sequence alignments for each search str along text file\n",
    "all_alignments = []\n",
    "alignment_scores = []\n",
    "k = 10 # Get Top K alignments\n",
    "w = (0.35, 0.35) # Alignment weight parameters likely hood of gaps.\n",
    "\n",
    "# Calculate cosine similarity between search phrase and text\n",
    "cos_dist = distance_matrix(out1, out2)\n",
    "\n",
    "# Calculate scoring matrix for sequence alignment\n",
    "score = scoring_matrix(cos_dist, wi=w[0], wj=w[1])\n",
    "\n",
    "# Find first k alignments of len > 1\n",
    "alignments = traceback(score, k=k)\n",
    "for j, _ in enumerate(alignments):\n",
    "    all_alignments.append(_)\n",
    "    alignment_scores.append(score_alignment(_, out1, out2, 1-(j/len(alignments))))\n",
    "\n",
    "k = len(alignments)\n",
    "# Sort Scored alignments\n",
    "sorted_scores = np.argsort(alignment_scores)[::-1]\n",
    "\n",
    "# Display results\n",
    "if True:\n",
    "    if k>1: print(\"Top \", k,':')\n",
    "    for i in range(k):\n",
    "        alignment = all_alignments[sorted_scores[i]]\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        l = -1\n",
    "        j = -1\n",
    "        for _ in reversed(alignment):\n",
    "            if _[0] != l:\n",
    "                ss1.append(tokens1[_[0]])\n",
    "                l = _[0]\n",
    "            else: ss1.append('GAP')\n",
    "            if _[1] != j:\n",
    "                ss2.append(tokens2[_[1]])\n",
    "                j = _[1]\n",
    "            else: ss2.append('GAP')\n",
    "        print('Match', i+1, ':', 'Score:',alignment_scores[sorted_scores[i]])\n",
    "        print(ss1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partially updated search\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize \n",
    "model = load_model()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def filter_sent(example_sent):  \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    punt = [\"!\",'#','$','&','(',')','*','+','-','.',':',';','<','=','>','?','@','[',']','^','_','`','{','|','}','~',',']\n",
    "    example_sent = example_sent.lower()\n",
    "    word_tokens = word_tokenize(example_sent) \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and not w in punt] \n",
    "\n",
    "    sentence = ''\n",
    "    for word in filtered_sentence:\n",
    "        sentence+=word+' '\n",
    "    return sentence\n",
    "\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output[0].detach()\n",
    "        \n",
    "    return hook\n",
    "model.bert.encoder.register_forward_hook(get_activation(\"encoder\")) \n",
    "\n",
    "\n",
    "def word_embedding(query_string): \n",
    "    model_input, att, compare, mask_indices, tokens1 = get_model_input(query_string, n_percent_mask=0.0)\n",
    "    out = model(model_input,attention_mask = att, masked_lm_labels = compare)\n",
    "    out1 = activation[\"encoder\"][0].numpy()[0:len(tokens1)]\n",
    "    hidden_states = out[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [128 x 13 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [13 x 768] tensor with each layer containing its respective embeddings of each token\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    embed = []\n",
    "    for x in token_vecs_sum[0:len(tokens1)]:\n",
    "        embed.append(x.numpy())\n",
    "    return embed, tokens1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_string = 'she has been complaining of diffuse abdominal pain'\n",
    "def is_SUICIDE(x, name, heatmap = False, export = False):\n",
    "    with torch.no_grad():\n",
    "        str1=query_string\n",
    "        out1, tokens1 = word_embedding(str1)\n",
    "\n",
    "        str2 = x\n",
    "        out3, tokens2 = word_embedding(str2)\n",
    "        \n",
    "#         str1=query_string\n",
    "#         str1 = filter_sent(str1)\n",
    "#         model_input, att, compare, mask_indices, tokens1 = get_model_input(str1, n_percent_mask=0.0)\n",
    "#         out = model(model_input,attention_mask = att, masked_lm_labels = compare)\n",
    "#         out1 = activation[\"encoder\"][0].numpy()[0:len(tokens1)]\n",
    "#         print(out1)\n",
    "\n",
    "#         str2 = x\n",
    "#         str2 = filter_sent(str2)\n",
    "#         model_input2, att2, compare2, mask_indices2, tokens2 = get_model_input(str2, n_percent_mask=0.0)\n",
    "#         out2 = model(model_input2,attention_mask = att2, masked_lm_labels = compare2)\n",
    "#         out3 = activation[\"encoder\"][0].numpy()[0:len(tokens2)]\n",
    "#         hidden2 = out2[2][-1][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    cos = cosine_distances(out1,out3)\n",
    "    cos = hmean(cos.flatten())\n",
    "\n",
    "    if not heatmap:\n",
    "        return tokens1, tokens2, cos\n",
    "    else:\n",
    "        data=cos\n",
    "        try:\n",
    "            fig = px.imshow(data,\n",
    "                            labels=dict(x='Sentence 1', y=\"Sentence 2\", color=\"Distances\"),\n",
    "                            x=tokens2,\n",
    "                            y=tokens1\n",
    "                           )\n",
    "        except:\n",
    "            fig = px.imshow(data,\n",
    "                            labels=dict(x='Sentence 1', y=\"Sentence 2\", color=\"Distances\"),\n",
    "                            x=tokens2[0:128],\n",
    "                            y=tokens1[0:128]\n",
    "                           )\n",
    "\n",
    "\n",
    "        fig.update_layout(font=dict(\n",
    "                size=8,\n",
    "            ))\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': \"Cosine Distances\",\n",
    "                'y':0.985,\n",
    "                'x':0.04,\n",
    "                'yanchor': 'top'})\n",
    "\n",
    "        fig.update_layout(\n",
    "            autosize=False,\n",
    "            width=400,\n",
    "            height=400)\n",
    "\n",
    "        fig.update_xaxes(side=\"top\")\n",
    "        fig.show()\n",
    "        \n",
    "        if export:\n",
    "            fig.write_html(name+'.html')\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "tokens1, tokens2, cos = is_SUICIDE(\"Abdominal pain may be related to alcohol consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D BERT embeddings vs Word2Vec visualization \n",
    "from bert_embedding import BertEmbedding\n",
    "embed = BertEmbedding()\n",
    "\n",
    "\n",
    "def get_visual_embs(sentence):\n",
    "    \"\"\"Get BERT embedding for the sentence,\n",
    "    project it to a 2D subspace where [CLS] is (1,0) and [SEP] is (0,1).\"\"\"\n",
    "    embs = embed([sentence], False)\n",
    "    tokens = embs[0][0]\n",
    "    embV = embs[0][1]\n",
    "    W = np.array(embV)\n",
    "    \n",
    "    B = np.array([embV[0], embV[-1]])\n",
    "    Bi = np.linalg.pinv(B.T)\n",
    "    Wp = np.matmul(Bi,W.T)\n",
    "\n",
    "    return Wp, tokens\n",
    "\n",
    "\n",
    "Wp, tokens =  get_visual_embs('The sky is blue today.')\n",
    "Wp[0,:]\n",
    "\n",
    "tokens\n",
    "\n",
    "Wp2, tokens2 = get_visual_embs(\"He used the lead in the pencil to write.\")\n",
    "\n",
    "Wp2\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "Wp, tokens =  get_visual_embs('They lead the basketball team.')\n",
    "\n",
    "Wp2, tokens2 = get_visual_embs(\"Lead is a common element.\")\n",
    "Wp3, tokens3 = get_visual_embs(\"They lead the volleyball team to victory.\")\n",
    "fig = go.Figure()\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print(tokens)\n",
    "# Add traces\n",
    "\n",
    "fig.add_trace(go.Scatter(x=Wp[0,:], y=Wp[1,:],\n",
    "                    mode='markers+lines+text',\n",
    "                    name='Lead(verb)',\n",
    "                    text = [None if x!='lead' else 'lead' for x in tokens],textposition=\"bottom center\"))\n",
    "fig.add_trace(go.Scatter(x=Wp2[0,:], y=Wp2[1,:],\n",
    "                    mode='markers+lines+text',\n",
    "                    name='Lead(noun)',\n",
    "                    text = [None if x!='lead' else 'lead' for x in tokens],textposition=\"bottom center\"))\n",
    "fig.add_trace(go.Scatter(x=Wp3[0,:], y=Wp3[1,:],\n",
    "                    mode='markers+lines+text',\n",
    "                    name='Lead(verb)',\n",
    "                    text = [None if x!='lead' else 'lead' for x in tokens],textposition=\"bottom center\"))\n",
    "fig.show()\n",
    "fig.write_html(\"embeddingsVis.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
